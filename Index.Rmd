---
title: "Potential Client Detection"
description: |
  An ANN aproach for customer Classification.
author:
  - name: Josep R.C.
    url: https://www.linkedin.com/in/josep%F0%9F%8C%AB-roman-cardell-414880184/
    affiliation: UIB Student
    affiliation_url: https://www.uib.cat/
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
    toc_depth: 3
    toc_float: true
    code_folding: true
    theme: theme1.css
    selfe_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

```{r ,code_folding = "Libraries"}
library(readr)
library(tidyverse)
library(magrittr)

library(neuralnet)

library(ggplot2)
library(rmarkdown)
```

# About the data

The dataset, **Bank Marketing**, is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (or not) subscribed. The classification goal is to **predict if the client will subscribe** a term deposit. 

Data was recorded from May 2008 to November 2010. 

## Feature Selection

From my lack of knowledge in terms of *Term Deposits* I first did a bit of research in order to better understand the matter of analysis and thus, be able to efficiently select the optimum variables. I focused my research on [Investopedia term deposit article](https://www.investopedia.com/terms/t/termdeposit.asp). 

The features are divided in:
  + **Personal and bank related client data**. I have decided to start with all variables as a profound description of the client is necessary in order to know if the person will be willing, or even able to invest in a term deposit.
  + **Last contact data**. Regarding the last call data, I believe it can be interesting to keep the month and day in which it occurred. My hypothesis is that there is a possible by which people in general fill more confident about their money in some periods of the year or the week. Also the duration of the call can be helpful. However, I've decided to not use a feature which labels the call as it was done via *telephone*, *cellular* or *unkown*, as a think it's redundant for this problem.
  + Other attributes. I have kept the four variables as a description of the relation between the client and the current and previous campaigns.
  
For a detailed description of all features visit the [metadata](https://github.com/Josep-at-work/Predicting-Customer-Subcription-ANN/blob/main/Data/bank-names.txt).

# Data Wrangling

First of all, I've loaded the dataset and removed the *contact* feature. Then, I've modified some data types. I've transformed all binary variables into 1 and 0 instead of *yes* and *no* respectively, and all character features into factors. Both of this changes will help on the visualization tasks and the training of the model. 

```{r Loading, code_folding = "Loading Data"}
data <- read_delim("Data/bank-full.csv", ";", escape_double = FALSE, trim_ws = TRUE)
data %<>% select(-contact)
str(data)
```

```{r, code_folding = "Summary"}
data$default <- ifelse(data$default == "yes", 1, 0)
data$housing <- ifelse(data$housing == "yes", 1, 0)
data$loan <- ifelse(data$loan == "yes", 1, 0)
data$y <- ifelse(data$y == "yes", 1, 0)
copy <- data #create a copy of the data 
data %<>% mutate(across(c(job, marital, education, month, poutcome,
                          default, housing, loan, y), as.factor))
summary(data)
```
From the target variable we can say that only 11.7% of the clients subscribed to the term deposit. 

## Missing values.

```{r}
data.frame("NA's" = colSums(is.na(data))) %>% paged_table()
```
There are no missing values in any of the features.

# Exploratory Data Analysis

Now it's time to study the data behavior and relations, as well as detect outliers and other specific values.

## Target variable

```{r Target, code_folding = "Target Feature"}
data %>%
  ggplot(aes(as.factor(y), fill = as.factor(y))) +
  geom_bar() +
  theme_minimal() +
  labs(x = "Target") +
  theme(legend.position = "none") +
  ggtitle("Target")

data.frame(prop.table(table(data$y))) %>% paged_table()
```

The target data in this study is heavily unbalanced.

Starting from the fact that only a 11.7% of the contacted clients subscribed to the campaign, the following plots aim to visualize the **normalized distribution** of the target variable among all categorical features:

```{r Categorical Features, code_folding = "Categorical Features"}
data %>%
  ggplot(aes(y, fill = job)) +
  geom_bar(position = "fill") +
  theme_minimal() +
  ggtitle("By job")

data %>%
  ggplot(aes(y, fill = marital)) +
  geom_bar(position = "fill") +
  theme_minimal() +
  ggtitle("By marital status")

data %>%
  ggplot(aes(y, fill = education)) +
  geom_bar(position = "fill") +
  theme_minimal() +
  ggtitle("By education")

# data %>%
#   ggplot(aes(y, fill = month)) +
#   geom_bar(position = "fill") +
#   theme_minimal() +
#   ggtitle("By month")

data %>%
  ggplot(aes(y, fill = poutcome)) +
  geom_bar(position = "fill") +
  theme_minimal() +
  ggtitle("By Previous Campaign's outcome")
```

From these normalized stacked plots there is little to say. There isn't much difference in the proportion of people of each category in both groups. The most relevant aspect is that the proportion of people who subscribed in previous campaigns is higher in the group of customers who actually subscribed to the current campaign of study.


```{r Boxplots, code_folding = "Continuous Features"}
data %>%
  ggplot(aes(x = y, y = age)) +
  geom_violin(aes(color = y), fill = 'grey', alpha=0.5) +
  geom_boxplot(width = 0.05) +
  theme_minimal() +
  theme(legend.position = "none") +
  ggtitle("Outcome By Age")

data %>%
  ggplot(aes(x = y, y = balance)) +
  geom_violin(aes(color = y), fill = 'grey', alpha=0.5, trim=F) +
  theme_minimal() +
  theme(legend.position = "none") +
  ggtitle("Outcome By Balance")

data %>%
  ggplot(aes(x = y, y = duration)) +
  geom_violin(aes(color = y), fill = 'grey', alpha=0.5, trim=F) +
  theme_minimal() +
  theme(legend.position = "none") +
  ggtitle("Outcome By Duration")


```

The target variable is a binary category, thus a huge advantage is that it's not possible to have outliers on the predicting variable. Then, by visualizing the distributions of the continuous features, we can see the points representing outliers regarding the respective distributions. However, the outliers on the dependent variables are proportionally equally distributed between both groups of clients. Hence, there is no need to modify those extreme values, which just represent minority groups of the population of this study.

Note that the distribution of subscribed customers ($y=1$), is wider at higher values of age and call duration compared to the other category

```{r Time Features, code_folding = "Time Features"}
data %>%
  ggplot(aes(month, fill = y)) +
  geom_bar(position = "stack") +
  theme_minimal() +
  ggtitle("Outcome By Month")

data %>%
  ggplot(aes(day, fill = y)) +
  geom_bar(position = "stack") +
  theme_minimal() +
  ggtitle("Outcome By day of the month")
```

```{r, code_folding = "Binary Features"}
data %>%
  ggplot(aes(housing, fill = y)) +
  geom_bar(position = "fill") +
  theme_minimal() +
  ggtitle("Housing Loan")

data %>%
  ggplot(aes(loan, fill = y)) +
  geom_bar(position = "fill") +
  theme_minimal() +
  ggtitle("Loan")
```

Customers with loans had lower tendency on subscribing to the term deposit.

Regarding the previous contacts, more than 75% of the customers where not contacted for previous campaigns. The following plot shows the trend of the remaining quantile. 

About the previous contacts from the campaign of study, at least 75% of the customers where contacted a maximum of 3 times.

```{r Previous contacts, code_folding = "Previous Contacts", layout = "l-page"}
data %>% filter(pdays>=0 & pdays<=400) %>%
  ggplot(aes(pdays, fill = y)) +
  geom_bar(position = "stack") +
  facet_grid(rows = vars(y), scales = "free") +
  theme_minimal() +
  theme(legend.position = "none") +
  ggtitle("Days from last contact for previous campaigns")

data %>% filter(campaign <= 10) %>%
  ggplot(aes(campaign, fill = y)) +
  geom_bar(position = "stack") +
  facet_grid(rows = vars(y), scales = "free") +
  theme_minimal() +
  theme(legend.position = "none") +
  ggtitle("Contacts during this campaign")
```

There are two picks in the number of customers who subscribed to the term deposite and had a contact for previous campaigns at around 90 and 170 previous days. 


# Models

The **target is a binary variable** which can take the values (0,1). Hence, the activation function will be the same in all models, which is going to be the logistic transfer function. This function is continuous so after the training the output values will need to be rounded so that they are either 0 or 1. 

Previous to training the models, I've processed the features as all input features of an artificial neural network must be numeric. Hence, I've transformed the categorical variables into features and subsequently to numbers. Now, the categorical variables still define different categories, yet instead of having a character labels, they have a number.

```{r, code_folding = "Preprocessing"}
model_data <- copy %>% 
  mutate(across(c(job, marital, education, month, poutcome), as.factor)) %>%
  mutate(across(c(job, marital, education, month, poutcome,
                          default, housing, loan, y), as.numeric))
```

Next step, is to split the data into training and testing in a 70/30 ratio.

```{r Split, code_folding = "Split"}
set.seed(10)
n = nrow(data)
m = ncol(data)
idx = sample(n, n*0.7)
train = model_data[idx, ]
test = model_data[idx, ]
summary(train); summary(test)
```

The following table is defined for a learning rate tuning that will be applied to the *backpropagation* models.

```{r, code_folding = "lr_tuning"}
a = c("logistic")
lr = c(0.005, 0.01, 0.05, 0.1)
results = matrix(ncol = length(lr), nrow = length(a))
colnames(results) = as.character(lr)
rownames(results) = a
```


### ANN 1

```{r M1Description}
data.frame(Parameter = c("Hidden Layers", "Learning Rate",
                                  "Algorithm", "Activation"),
                    Values = c(1, "tunning", "backprop", "logistic")) %>% paged_table()
```


```{r}
for (l in 1:length(lr)){
  set.seed(2805)
  ann1 <- neuralnet(formula = y ~ ., data = train,
                  hidden = c(4), learningrate = lr[l],
                  algorithm = "backprop", act.fct = "logistic",
                  err.fct = "sse", 
                  linear.output = F)
  if (is.null(ann1$result.matrix[1])){
    results[a, l] = NA
  }
  else {
    results[a, l] = ann1$result.matrix[1]
  }
}
results %>% as.data.frame() %>% paged_table()
```
```{r}
pred1 <- compute(ann1, test[-m])
Acc1 = mean(test$y == round(pred1$net.result))
table(round(pred1$net.result), test$y)
```

### ANN2

```{r M2Description}
data.frame(Parameter = c("Hidden Layers", "Learning Rate",
                                  "Algorithm", "Activation"),
                    Values = c(3, "tunning", "backprop", "logistic")) %>% paged_table()
```


```{r}
for (l in 1:length(lr)){
  set.seed(2805)
  ann2 <- neuralnet(formula = y ~ ., data = train,
                  hidden = c(2, 4, 3), learningrate = lr[l],
                  algorithm = "backprop", act.fct = "logistic",
                  err.fct = "sse", 
                  linear.output = F)
  if (is.null(ann2$result.matrix[1])){
    results[a, l] = NA
  }
  else {
    results[a, l] = ann2$result.matrix[1]
  }
}
results %>% as.data.frame() %>% paged_table()
```

```{r}
pred2 <- compute(ann2, test[-m])
Acc1 = mean(test$y == round(pred2$net.result))
table(round(pred2$net.result), test$y)
```

Eventhough this model is more complex than the first one, with 3 hidden layers and 9 neurons, still the performance of the model is poor.  

### ANN3

For this third model I'll try out a different propagation algorithm resilient backpropagation.

```{r M3Description}
data.frame(Parameter = c("Hidden Layers", "Learning Rate",
                                  "Algorithm", "Activation", 
                         "Threshold"),
                    Values = c(3, "0.05<lr<1", "resilient backprop", "logistic", 1)) %>% paged_table()
```

```{r}
ann3 <- neuralnet(formula = y ~ ., data = train,
                  hidden = c(2, 4, 2), learningrate = 0.3,
                  # learningrate.limit = c(0.05, 1),
                  algorithm = "rprop+", act.fct = "logistic",
                  err.fct = "sse", threshold = 1, 
                  linear.output = F)

pred3 <- compute(ann3, test[-m])
Acc3 = mean(test$y == round(pred3$net.result))
table(round(pred3$net.result), test$y)
Acc3_2 = mean(test$y == ifelse(pred3$net.result<=0.073997670, 0, 1))
table(ifelse(pred3$net.result<=0.073997670, 0, 1), test$y)
confusionMatrix(data = as.factor(ifelse(pred3$net.result<=0.073997670, 0, 1)), 
                reference = as.factor(test$y))
```

### Oversampling  

As seen during the EDA, the target variable is heavily unbalanced. Hence, I've decided to compensate it by duplicating the observations of the minority group in the training set and then shuffeling the whole dataset and training the neural network with the outcome of this  process.

```{r Oversampling, code_folding = "Oversampling"}
minority <- train %>% filter(y==1)
train2 <- rbind(train, minority) %>% sample_frac()
prop.table(table(train2$y)); prop.table(table(train$y))
```

```{r}
ann4 <- neuralnet(formula = y ~ ., data = train2,
                  hidden = c(2, 4, 2), learningrate = 0.5,
                  # learningrate.limit = c(0.05, 1),
                  algorithm = "backprop", act.fct = "logistic",
                  err.fct = "sse", threshold = 1, 
                  linear.output = F)
ann4$result.matrix[1]
pred4 <- compute(ann4, test[-m])
Acc4 = mean(test$y == round(pred4$net.result))
table(round(pred4$net.result), test$y)
```


# References 

[Moro et al., 2011] **S. Moro, R. Laureano and P. Cortez.** *Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology.* 
  In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling Conference - ESM'2011, pp. 117-121, Guimarães, Portugal, October, 2011. EUROSIS.
  
[Term Deposit, Fixed Income Essentials, Incesopedia](https://www.investopedia.com/terms/t/termdeposit.asp)



